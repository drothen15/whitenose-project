{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e7ae80-166c-4aaa-97f0-71c8e97255e9",
   "metadata": {},
   "source": [
    "# Indicator Species Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84819c10-bd09-423d-bc40-216c37b56472",
   "metadata": {},
   "source": [
    "## Calulating ISA values and running 4999 Monte Carlo Permutations to Determine Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217252b3-9abf-4c0d-9209-f5b03b8cb7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_indicator_value_stats(species_abundances_in_group, \n",
    "                                    species_abundances_all_samples,\n",
    "                                    group_sample_indices, \n",
    "                                    all_sample_group_assignments):\n",
    "    \"\"\"\n",
    "    Calculates specificity (A), fidelity (B), and IndVal for a single species in a target group.\n",
    "\n",
    "    Args:\n",
    "        species_abundances_in_group (pd.Series): Abundances of the species in samples belonging to the target group.\n",
    "        species_abundances_all_samples (pd.Series): Abundances of the species across all samples.\n",
    "        group_sample_indices (list): List of sample indices belonging to the target group.\n",
    "        all_sample_group_assignments (pd.Series): Series mapping all sample indices to their group labels.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (specificity_A, fidelity_B, indval)\n",
    "    \"\"\"\n",
    "    # Specificity (A_ij)\n",
    "    # Mean abundance of species i in sites of group j / sum of mean abundances of species i in all groups\n",
    "    mean_abundance_in_target_group = species_abundances_in_group.mean()\n",
    "    \n",
    "    sum_of_mean_abundances_across_groups = 0\n",
    "    unique_groups = all_sample_group_assignments.unique()\n",
    "    for grp in unique_groups:\n",
    "        grp_samples_mask = (all_sample_group_assignments == grp)\n",
    "        mean_abund_in_grp = species_abundances_all_samples[grp_samples_mask].mean()\n",
    "        if pd.isna(mean_abund_in_grp): # Handle case where a group might have no samples after filtering\n",
    "            mean_abund_in_grp = 0\n",
    "        sum_of_mean_abundances_across_groups += mean_abund_in_grp\n",
    "        \n",
    "    specificity_A = 0\n",
    "    if sum_of_mean_abundances_across_groups > 0:\n",
    "        specificity_A = mean_abundance_in_target_group / sum_of_mean_abundances_across_groups\n",
    "    if pd.isna(specificity_A): specificity_A = 0\n",
    "\n",
    "\n",
    "    # Fidelity (B_ij)\n",
    "    # Number of sites in group j where species i is present / total number of sites in group j\n",
    "    num_sites_in_group_with_species = (species_abundances_in_group > 0).sum()\n",
    "    total_sites_in_group = len(group_sample_indices)\n",
    "    \n",
    "    fidelity_B = 0\n",
    "    if total_sites_in_group > 0:\n",
    "        fidelity_B = num_sites_in_group_with_species / total_sites_in_group\n",
    "    if pd.isna(fidelity_B): fidelity_B = 0\n",
    "\n",
    "    indval = specificity_A * fidelity_B\n",
    "    return specificity_A, fidelity_B, indval\n",
    "\n",
    "\n",
    "def run_indicator_species_analysis(\n",
    "        otu_table_path, \n",
    "        metadata_path, \n",
    "        output_path,\n",
    "        metadata_sample_id_col=\"#SampleID\", \n",
    "        grouping_variable_col=\"DiseaseStatus\",\n",
    "        num_permutations=4999,\n",
    "        taxonomy_cols_count=9 # Number of taxonomy columns after OTU_ID\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Performs Indicator Species Analysis using IndVal and Monte Carlo permutations.\n",
    "    \"\"\"\n",
    "    print(f\"Loading OTU table from: {otu_table_path}\")\n",
    "    try:\n",
    "        df_otu_full = pd.read_csv(otu_table_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: OTU table file '{otu_table_path}' not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading OTU table: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading metadata from: {metadata_path}\")\n",
    "    try:\n",
    "        df_metadata = pd.read_csv(metadata_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Metadata file '{metadata_path}' not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        return\n",
    "\n",
    "    if df_otu_full.empty or df_metadata.empty:\n",
    "        print(\"OTU table or metadata is empty. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # --- Data Preprocessing ---\n",
    "    # OTU Table\n",
    "    otu_id_col = df_otu_full.columns[0]\n",
    "    print(f\"Using '{otu_id_col}' as OTU/Taxon identifier column.\")\n",
    "    \n",
    "    # Identify sample columns (assumed to be after OTU_ID and taxonomy columns)\n",
    "    # Using regex for A##, B## pattern, robust to other non-matching columns\n",
    "    sample_col_pattern = re.compile(r'^[A-Z]\\d{2}$')\n",
    "    all_columns = df_otu_full.columns.tolist()\n",
    "    \n",
    "    # Determine start of sample columns: first column after OTU_ID + taxonomy_cols_count\n",
    "    # Or, more robustly, filter by pattern if taxonomy columns are not strictly fixed\n",
    "    potential_sample_cols_start_index = 1 + taxonomy_cols_count \n",
    "    sample_cols = [col for col in all_columns[potential_sample_cols_start_index:] if sample_col_pattern.match(col)]\n",
    "    \n",
    "    if not sample_cols:\n",
    "        # Fallback if the index-based method fails, try to find sample columns anywhere\n",
    "        sample_cols = [col for col in all_columns if sample_col_pattern.match(col)]\n",
    "        if not sample_cols:\n",
    "            print(\"Error: No sample columns found matching the pattern (e.g., A01, B12). Please check OTU table format.\")\n",
    "            return\n",
    "    print(f\"Identified {len(sample_cols)} sample columns in OTU table.\")\n",
    "\n",
    "    df_otu_abundances = df_otu_full.set_index(otu_id_col)[sample_cols]\n",
    "    df_otu_abundances = df_otu_abundances.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    # Keep taxonomy for final output\n",
    "    taxonomy_map = df_otu_full.set_index(otu_id_col).iloc[:, 0:taxonomy_cols_count]\n",
    "\n",
    "\n",
    "    # Metadata\n",
    "    if metadata_sample_id_col not in df_metadata.columns:\n",
    "        print(f\"Error: Metadata sample ID column '{metadata_sample_id_col}' not found.\")\n",
    "        return\n",
    "    if grouping_variable_col not in df_metadata.columns:\n",
    "        print(f\"Error: Grouping variable column '{grouping_variable_col}' not found in metadata.\")\n",
    "        return\n",
    "    \n",
    "    df_metadata[metadata_sample_id_col] = df_metadata[metadata_sample_id_col].astype(str)\n",
    "    df_metadata = df_metadata.set_index(metadata_sample_id_col)\n",
    "    \n",
    "    # Align samples\n",
    "    common_samples = list(set(df_otu_abundances.columns) & set(df_metadata.index))\n",
    "    if not common_samples:\n",
    "        print(\"Error: No common samples found between OTU table and metadata. Check sample IDs.\")\n",
    "        return\n",
    "    \n",
    "    df_otu_aligned = df_otu_abundances[common_samples]\n",
    "    df_metadata_aligned = df_metadata.loc[common_samples]\n",
    "    \n",
    "    # Ensure group assignments are a Series with sample IDs as index\n",
    "    group_assignments = df_metadata_aligned[grouping_variable_col].astype('category')\n",
    "    unique_groups = group_assignments.cat.categories.tolist()\n",
    "    print(f\"Found {len(unique_groups)} unique groups: {unique_groups}\")\n",
    "    if len(unique_groups) < 2:\n",
    "        print(\"Error: Less than 2 groups found. Indicator Species Analysis requires at least 2 groups.\")\n",
    "        return\n",
    "\n",
    "    # --- Calculate Observed Indicator Values ---\n",
    "    print(\"\\nCalculating observed indicator values...\")\n",
    "    observed_indvals = pd.DataFrame(index=df_otu_aligned.index, columns=unique_groups, dtype=float)\n",
    "    \n",
    "    for species_idx, species_id in enumerate(df_otu_aligned.index):\n",
    "        if (species_idx + 1) % 100 == 0:\n",
    "            print(f\"  Processing observed IndVal for species {species_idx + 1}/{len(df_otu_aligned.index)}\")\n",
    "        species_data = df_otu_aligned.loc[species_id]\n",
    "        for group_label in unique_groups:\n",
    "            group_samples_mask = (group_assignments == group_label)\n",
    "            group_sample_ids = group_assignments[group_samples_mask].index.tolist() # Get actual sample IDs for the group\n",
    "            \n",
    "            if not group_sample_ids: # If a group has no samples after alignment\n",
    "                observed_indvals.loc[species_id, group_label] = 0.0\n",
    "                continue\n",
    "\n",
    "            species_abund_in_grp = species_data[group_sample_ids]\n",
    "            \n",
    "            _, _, indval = calculate_indicator_value_stats(\n",
    "                species_abund_in_grp,\n",
    "                species_data, # Pass all samples for this species\n",
    "                group_sample_ids, # Pass actual sample IDs for the group\n",
    "                group_assignments # Pass all group assignments\n",
    "            )\n",
    "            observed_indvals.loc[species_id, group_label] = indval\n",
    "            \n",
    "    observed_max_indvals = observed_indvals.max(axis=1)\n",
    "    observed_associated_group = observed_indvals.idxmax(axis=1)\n",
    "\n",
    "    # --- Permutation Test ---\n",
    "    print(f\"\\nStarting permutation test ({num_permutations} permutations)...\")\n",
    "    # Stores how many times permuted max IndVal was >= observed max IndVal for each species\n",
    "    perm_greater_counts = pd.Series(0, index=df_otu_aligned.index, dtype=int)\n",
    "\n",
    "    original_group_assignments_array = group_assignments.values # For shuffling\n",
    "    \n",
    "    for i_perm in range(num_permutations):\n",
    "        if (i_perm + 1) % 100 == 0:\n",
    "            print(f\"  Permutation {i_perm + 1}/{num_permutations}\")\n",
    "        \n",
    "        permuted_group_assignments_array = np.random.permutation(original_group_assignments_array)\n",
    "        permuted_group_assignments = pd.Series(permuted_group_assignments_array, index=group_assignments.index)\n",
    "        \n",
    "        permuted_indvals_for_iter = pd.DataFrame(index=df_otu_aligned.index, columns=unique_groups, dtype=float)\n",
    "\n",
    "        for species_id in df_otu_aligned.index:\n",
    "            species_data = df_otu_aligned.loc[species_id]\n",
    "            for group_label in unique_groups:\n",
    "                group_samples_mask_perm = (permuted_group_assignments == group_label)\n",
    "                group_sample_ids_perm = permuted_group_assignments[group_samples_mask_perm].index.tolist()\n",
    "\n",
    "                if not group_sample_ids_perm:\n",
    "                    permuted_indvals_for_iter.loc[species_id, group_label] = 0.0\n",
    "                    continue\n",
    "                \n",
    "                species_abund_in_grp_perm = species_data[group_sample_ids_perm]\n",
    "                \n",
    "                _, _, indval_perm = calculate_indicator_value_stats(\n",
    "                    species_abund_in_grp_perm,\n",
    "                    species_data,\n",
    "                    group_sample_ids_perm,\n",
    "                    permuted_group_assignments # Use permuted assignments here\n",
    "                )\n",
    "                permuted_indvals_for_iter.loc[species_id, group_label] = indval_perm\n",
    "        \n",
    "        permuted_max_indvals_for_iter = permuted_indvals_for_iter.max(axis=1)\n",
    "        perm_greater_counts += (permuted_max_indvals_for_iter >= observed_max_indvals).astype(int)\n",
    "\n",
    "    # Calculate p-values\n",
    "    p_values = (perm_greater_counts + 1) / (num_permutations + 1)\n",
    "\n",
    "    # --- Prepare Results ---\n",
    "    results_df = pd.DataFrame({\n",
    "        'Taxon_ID': observed_max_indvals.index,\n",
    "        'Associated_Group': observed_associated_group,\n",
    "        'Max_IndVal': observed_max_indvals,\n",
    "        'Permutation_P_Value': p_values\n",
    "    })\n",
    "    \n",
    "    # Add original taxonomy information\n",
    "    results_df = results_df.join(taxonomy_map, on='Taxon_ID')\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    cols_order = ['Taxon_ID'] + taxonomy_map.columns.tolist() + ['Associated_Group', 'Max_IndVal', 'Permutation_P_Value']\n",
    "    results_df = results_df[cols_order]\n",
    "    \n",
    "    results_df = results_df.sort_values(by=['Permutation_P_Value', 'Max_IndVal'], ascending=[True, False])\n",
    "\n",
    "    print(f\"\\nSaving results to: {output_path}\")\n",
    "    try:\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(\"Indicator Species Analysis complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    otu_table_file = \"otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains_combined_by_genus_keep_blanks_species_column_removed.csv\"\n",
    "    metadata_file = \"otu_metadata_uclust_updated_taxonomy_05162025.csv\" # Make sure this is the correct metadata file\n",
    "    \n",
    "    # Column names in your metadata file\n",
    "    meta_sample_id_col = \"SampleID\" # Or \"SampleID\", \"sample_id\", etc.\n",
    "    meta_grouping_col = \"DiseaseStatus\"\n",
    "    \n",
    "    # Number of initial columns in OTU table that are taxonomy (after the first OTU_ID column)\n",
    "    # Domain, Super Kingdom, Kingdom, Phylum, Class, Order, Family, Genus = 8 columns\n",
    "    num_taxonomy_cols = 8 \n",
    "    \n",
    "    num_perms = 4999 # As requested\n",
    "    \n",
    "    output_results_file = \"indicator_species_analysis_results.csv\"\n",
    "    \n",
    "    # --- Run Analysis ---\n",
    "    if not os.path.exists(otu_table_file):\n",
    "        print(f\"CRITICAL ERROR: OTU table file '{otu_table_file}' not found.\")\n",
    "    elif not os.path.exists(metadata_file):\n",
    "        print(f\"CRITICAL ERROR: Metadata file '{metadata_file}' not found.\")\n",
    "    else:\n",
    "        run_indicator_species_analysis(\n",
    "            otu_table_path=otu_table_file,\n",
    "            metadata_path=metadata_file,\n",
    "            output_path=output_results_file,\n",
    "            metadata_sample_id_col=meta_sample_id_col,\n",
    "            grouping_variable_col=meta_grouping_col,\n",
    "            num_permutations=num_perms,\n",
    "            taxonomy_cols_count=num_taxonomy_cols\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6225c7a4-9435-433e-a52f-e39e273a4d52",
   "metadata": {},
   "source": [
    "## Creating Dot Plot from ISA output.\n",
    "- Including at least one significant taxa from each DiseaseStatus Group\n",
    "- Remaining plotted taxa need at least a ISA of 0.2 and be significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03664a-6549-4456-a0ee-385cf1f3d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D \n",
    "\n",
    "def get_most_specific_taxon(row, tax_levels, uninformative_strings, fallback_id_str):\n",
    "    \"\"\"\n",
    "    Finds the most specific non-blank taxonomic assignment from a list of levels.\n",
    "    Returns the name and the level string.\n",
    "    \"\"\"\n",
    "    for level in tax_levels: \n",
    "        value = row[level]\n",
    "        if pd.notna(value):\n",
    "            value_str = str(value).strip()\n",
    "            if value_str.lower() not in uninformative_strings and value_str != \"\":\n",
    "                return value_str, level \n",
    "    return fallback_id_str, \"ID\" \n",
    "\n",
    "def plot_significant_indicator_species(\n",
    "    results_csv_path=\"indicator_species_analysis_results.csv\",\n",
    "    output_plot_path=\"significant_indicator_species_dot_plot.png\",\n",
    "    p_value_threshold=0.05,\n",
    "    indval_threshold=0.2,\n",
    "    no_disease_group_name=\"No Disease Present\" \n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a dot plot of significant indicator species.\n",
    "    Special filtering for 'no_disease_group_name'.\n",
    "    Custom p-value size legend with example dots, positioned correctly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_results = pd.read_csv(results_csv_path)\n",
    "        print(f\"Successfully loaded indicator species results: '{results_csv_path}' (Shape: {df_results.shape})\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Results file '{results_csv_path}' not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    if df_results.empty:\n",
    "        print(\"Results DataFrame is empty. Cannot generate plot.\")\n",
    "        return\n",
    "\n",
    "    required_cols = ['Taxon_ID', 'Domain', 'Genus', 'Family', 'Order', 'Class', 'Phylum', \n",
    "                     'Kingdom', 'Super Kingdom', 'Associated_Group', 'Max_IndVal', 'Permutation_P_Value']\n",
    "    missing_cols = [col for col in required_cols if col not in df_results.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: The following required columns are missing from '{results_csv_path}': {', '.join(missing_cols)}\")\n",
    "        return\n",
    "\n",
    "    df_significant_p = df_results[df_results['Permutation_P_Value'] < p_value_threshold].copy()\n",
    "\n",
    "    if df_significant_p.empty:\n",
    "        print(f\"No significant indicators found with P-value < {p_value_threshold}. No plot will be generated.\")\n",
    "        return\n",
    "    \n",
    "    df_no_disease_hits = df_significant_p[df_significant_p['Associated_Group'] == no_disease_group_name]\n",
    "    df_other_groups_hits = df_significant_p[\n",
    "        (df_significant_p['Associated_Group'] != no_disease_group_name) &\n",
    "        (df_significant_p['Max_IndVal'] > indval_threshold)\n",
    "    ]\n",
    "    \n",
    "    df_significant = pd.concat([df_no_disease_hits, df_other_groups_hits])\n",
    "    \n",
    "    if df_significant.empty:\n",
    "        print(f\"No significant indicators found meeting the specified P-value and IndVal criteria (with exception for '{no_disease_group_name}'). No plot will be generated.\")\n",
    "        return\n",
    "    \n",
    "    num_no_disease_included_conditionally = 0\n",
    "    if no_disease_group_name in df_significant_p['Associated_Group'].unique():\n",
    "        num_no_disease_included_conditionally = len(\n",
    "            df_significant_p[\n",
    "                (df_significant_p['Associated_Group'] == no_disease_group_name) &\n",
    "                (df_significant_p['Max_IndVal'] <= indval_threshold) \n",
    "            ]\n",
    "        )\n",
    "    print(f\"Found {len(df_significant)} significant indicators to plot.\")\n",
    "    if num_no_disease_included_conditionally > 0:\n",
    "         print(f\"  (Including {num_no_disease_included_conditionally} significant hits from '{no_disease_group_name}' with IndVal <= {indval_threshold})\")\n",
    "\n",
    "    epsilon = 1e-300 \n",
    "    df_significant['P_Value_Size_Metric'] = -np.log10(df_significant['Permutation_P_Value'] + epsilon)\n",
    "\n",
    "    taxonomic_hierarchy = ['Genus', 'Family', 'Order', 'Class', 'Phylum', 'Kingdom', 'Super Kingdom'] ## because all like genera were combined earlier species assignment doesn't mean anything now.\n",
    "    uninformative_tax_strings = [\n",
    "        '', 'nan', 'none', 'na', '<na>', 'unassigned', 'unclassified', \n",
    "        'unknown', 'no blast hit', 'incertae sedis', 'metazoa', \n",
    "        'no classification', 'unknown_domain' \n",
    "    ]\n",
    "    uninformative_tax_strings = [s.lower() for s in uninformative_tax_strings]\n",
    "\n",
    "    display_labels = []\n",
    "    for index, row in df_significant.iterrows():\n",
    "        domain_val_raw = row['Domain']\n",
    "        domain_val_str = str(domain_val_raw).strip()\n",
    "        if pd.isna(domain_val_raw) or domain_val_str.lower() in uninformative_tax_strings or domain_val_str == \"\":\n",
    "            domain_prefix = \"Unknown_Domain\"\n",
    "        else:\n",
    "            domain_prefix = domain_val_str\n",
    "        \n",
    "        most_specific_name, specific_level_name = get_most_specific_taxon(row, taxonomic_hierarchy, uninformative_tax_strings, str(row['Taxon_ID']))\n",
    "        \n",
    "        if specific_level_name == \"ID\":\n",
    "            if domain_prefix == \"Unknown_Domain\":\n",
    "                final_label = f\"{most_specific_name} (ID)\" \n",
    "            else:\n",
    "                final_label = f\"{domain_prefix}: {most_specific_name} (ID)\"\n",
    "        else:\n",
    "            final_label = f\"{domain_prefix}: {most_specific_name} ({specific_level_name})\"\n",
    "        display_labels.append(final_label)\n",
    "        \n",
    "    df_significant['Display_Label'] = display_labels\n",
    "\n",
    "    df_significant_sorted = df_significant.sort_values(\n",
    "        by=['Associated_Group', 'Max_IndVal'],\n",
    "        ascending=[True, True] \n",
    "    ).reset_index(drop=True) \n",
    "\n",
    "    if df_significant_sorted.empty:\n",
    "        print(\"No data to plot after processing labels and sorting.\")\n",
    "        return\n",
    "\n",
    "    plt.style.use('seaborn-v0_8-pastel') \n",
    "    \n",
    "    num_indicators = len(df_significant_sorted)\n",
    "    fig_height = max(8, num_indicators * 0.4) \n",
    "    fig_width = 12 \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height)) \n",
    "\n",
    "    min_dot_size_plot, max_dot_size_plot = 50, 450\n",
    "    \n",
    "    dot_plot = sns.scatterplot(\n",
    "        y=df_significant_sorted.index, \n",
    "        x='Max_IndVal',\n",
    "        hue='Associated_Group',\n",
    "        size='P_Value_Size_Metric',  \n",
    "        sizes=(min_dot_size_plot, max_dot_size_plot),  \n",
    "        data=df_significant_sorted,\n",
    "        palette=\"pastel\", \n",
    "        ax=ax, \n",
    "        legend=False \n",
    "    )\n",
    "\n",
    "    for i in range(len(df_significant_sorted)):\n",
    "        hue_categories = df_significant_sorted['Associated_Group'].astype('category').cat.categories\n",
    "        palette_colors = sns.color_palette(\"pastel\", n_colors=len(hue_categories)) \n",
    "        color_map = dict(zip(hue_categories, palette_colors))\n",
    "        point_color = color_map[df_significant_sorted['Associated_Group'].iloc[i]]\n",
    "\n",
    "        plt.hlines(y=i, xmin=0, xmax=df_significant_sorted['Max_IndVal'].iloc[i], \n",
    "                   color=point_color, \n",
    "                   alpha=0.7, \n",
    "                   linewidth=1.5)\n",
    "\n",
    "    plt.yticks(ticks=df_significant_sorted.index, labels=df_significant_sorted['Display_Label'], fontsize=9)\n",
    "    plt.xlabel('Max Indicator Value (Max_IndVal)', fontsize=12)\n",
    "    plt.ylabel('Significant Taxa (Domain: Most Specific Level (Rank))', fontsize=12) \n",
    "    plt.title(f'Significant Indicator Taxa (P < {p_value_threshold}) by {df_significant_sorted[\"Associated_Group\"].name}', fontsize=14)\n",
    "    \n",
    "    plt.xlim(0, 1.0) \n",
    "    \n",
    "    # --- Custom Legend Handling ---\n",
    "    # 1. Hue Legend (Associated Group)\n",
    "    hue_handles, hue_labels_list = [], [] # Renamed to avoid conflict\n",
    "    hue_order = df_significant_sorted['Associated_Group'].astype('category').cat.categories.tolist()\n",
    "    palette_for_legend = sns.color_palette(\"pastel\", n_colors=len(hue_order))\n",
    "    hue_color_map_legend = dict(zip(hue_order, palette_for_legend))\n",
    "\n",
    "    for group_name in hue_order:\n",
    "        hue_handles.append(Line2D([0], [0], marker='o', color='w', \n",
    "                              markerfacecolor=hue_color_map_legend[group_name], markersize=8, label=group_name))\n",
    "        hue_labels_list.append(group_name)\n",
    "    \n",
    "    # Place the hue legend using ax.legend, its bbox_to_anchor is relative to the axes.\n",
    "    hue_legend = ax.legend(hue_handles, hue_labels_list, title='Associated Group', \n",
    "                           bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.,\n",
    "                           frameon=True, facecolor='white', edgecolor='grey') \n",
    "    # ax.add_artist(hue_legend) # ax.legend() already adds it.\n",
    "\n",
    "    # Force a draw of the canvas to ensure legend positions are finalized\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "    # 2. P-value Size Legend (Custom with dots)\n",
    "    # Get the bounding box of the hue_legend IN FIGURE COORDINATES\n",
    "    hue_legend_bbox_fig = hue_legend.get_window_extent().transformed(fig.transFigure.inverted())\n",
    "    \n",
    "    min_p_val_plotted = df_significant_sorted['Permutation_P_Value'].min()\n",
    "    max_p_val_plotted = df_significant_sorted['Permutation_P_Value'].max()\n",
    "    \n",
    "    min_p_size_metric = -np.log10(min_p_val_plotted + epsilon)\n",
    "    max_p_size_metric = -np.log10(max_p_val_plotted + epsilon)\n",
    "    \n",
    "    overall_min_metric = df_significant_sorted['P_Value_Size_Metric'].min()\n",
    "    overall_max_metric = df_significant_sorted['P_Value_Size_Metric'].max()\n",
    "\n",
    "    def scale_p_metric_to_dot_size(p_metric, overall_min, overall_max, size_min, size_max):\n",
    "        if overall_max == overall_min: \n",
    "            return (size_min + size_max) / 2\n",
    "        p_metric_clipped = np.clip(p_metric, overall_min, overall_max)\n",
    "        return size_min + ((p_metric_clipped - overall_min) / (overall_max - overall_min)) * (size_max - size_min)\n",
    "\n",
    "    size_for_min_p = scale_p_metric_to_dot_size(min_p_size_metric, overall_min_metric, overall_max_metric, min_dot_size_plot, max_dot_size_plot)\n",
    "    size_for_max_p = scale_p_metric_to_dot_size(max_p_size_metric, overall_min_metric, overall_max_metric, min_dot_size_plot, max_dot_size_plot)\n",
    "\n",
    "    legend_p_handles = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=np.sqrt(size_for_min_p), label=f\"p = {min_p_val_plotted:.2e}\"),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=np.sqrt(size_for_max_p), label=f\"p = {max_p_val_plotted:.2e}\")\n",
    "    ]\n",
    "    \n",
    "    # Position the p-value legend using fig.legend, its bbox_to_anchor is relative to the FIGURE.\n",
    "    # x0 of hue_legend_bbox_fig is the left edge of the hue legend in figure coordinates.\n",
    "    # y0 of hue_legend_bbox_fig is the bottom edge of the hue legend in figure coordinates.\n",
    "    # We want the p_value_legend's 'upper left' corner to be at (hue_legend_left, below_hue_legend_bottom)\n",
    "    p_value_legend_anchor_x = hue_legend_bbox_fig.x0 - 0.25\n",
    "    p_value_legend_anchor_y = hue_legend_bbox_fig.y0 # Adjust this 0.02 for vertical spacing\n",
    "\n",
    "    p_value_legend = fig.legend(handles=legend_p_handles, \n",
    "                                title=\"P-value (Dot Size)\",\n",
    "                                loc='upper left', # Anchor point of the p_value_legend box\n",
    "                                bbox_to_anchor=(p_value_legend_anchor_x, p_value_legend_anchor_y), \n",
    "                                borderaxespad=0.,\n",
    "                                frameon=True, facecolor='white', edgecolor='grey'\n",
    "                               )\n",
    "\n",
    "    # Adjust overall layout to make space for legends on the right\n",
    "    plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
    "\n",
    "    try:\n",
    "        plt.savefig(output_plot_path, bbox_inches='tight', dpi=600)\n",
    "        print(f\"\\nDot plot of significant indicators saved to '{output_plot_path}'\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    indicator_results_file = \"indicator_species_analysis_results.csv\" \n",
    "    plot_output_file = \"significant_indicators_dot_plot_refined_legend_placement.png\" \n",
    "    \n",
    "    min_indval_to_plot = 0.2\n",
    "    no_disease_group = \"No Disease Present\" \n",
    "    \n",
    "    if not os.path.exists(indicator_results_file):\n",
    "        print(f\"CRITICAL ERROR: The indicator results file '{indicator_results_file}' was not found.\")\n",
    "    else:\n",
    "        plot_significant_indicator_species(\n",
    "            results_csv_path=indicator_results_file,\n",
    "            output_plot_path=plot_output_file,\n",
    "            p_value_threshold=0.05,\n",
    "            indval_threshold=min_indval_to_plot,\n",
    "            no_disease_group_name=no_disease_group\n",
    "        )\n",
    "        print(\"\\nDot plot generation script finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
