{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3a795ac1-2b3d-4135-b152-c9e89650bbf8",
   "metadata": {},
   "source": [
    "### Summary ###\n",
    "Notebook contains three scripts that parse the otu table.\n",
    "- 1. Removes Singlets, taxa that only appear in one sample\n",
    "- 2. Removing Unclassified and Metazoa, used in preparing the bat microbiome otu table\n",
    "- 3. Combining like genera together and adding their counts together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759038f3-ebae-4033-863b-59b33eb6c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import programs\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ed4cb-4cdb-41a9-b25f-27ab925e5b24",
   "metadata": {},
   "source": [
    "# Removing Singlets from the data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4250c220-dc2c-4507-8b36-4a11e8eea343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OTU table processing for: otu_table_uclust_with_updated_taxonomy_05172025_78bats.csv (as standalone script)\n",
      "Filtered output will be saved to: otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats.csv\n",
      "Removed rows will be logged to: otu_table_uclust_with_updated_taxonomy_05172025_78bats_removed_rows.log_78bats.csv\n",
      "Identified columns for summing: ['A01', 'A02', 'A03', 'A04', 'A05', 'A08', 'A09', 'A10', 'A12', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B08', 'B09', 'B10', 'B12', 'C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C12', 'D01', 'D02', 'D03', 'D04', 'D05', 'D06', 'D07', 'D09', 'D10', 'D11', 'D12', 'E01', 'E02', 'E03', 'E04', 'E06', 'E07', 'E08', 'E09', 'E10', 'E11', 'E12', 'F01', 'F02', 'F03', 'F04', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11', 'G01', 'G02', 'G03', 'G04', 'G07', 'G08', 'G09', 'G10', 'G11', 'H01', 'H02', 'H03', 'H04', 'H07', 'H08', 'H09', 'H11']\n",
      "\n",
      "Processing complete for 'otu_table_uclust_with_updated_taxonomy_05172025_78bats.csv':\n",
      "  Total data rows processed (excluding header): 8078\n",
      "  Rows kept and written to 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats.csv': 1939\n",
      "  Rows removed and written to 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_removed_rows.log_78bats.csv': 6139\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_otu_table(input_csv_path, output_csv_path, log_csv_path, sum_threshold=2):\n",
    "    \"\"\"\n",
    "    Parses a CSV file, sums columns matching a specific pattern (e.g., A01, B12),\n",
    "    and filters rows based on this sum.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file.\n",
    "        output_csv_path (str): Path to write the filtered CSV data.\n",
    "        log_csv_path (str): Path to write the removed rows.\n",
    "        sum_threshold (int): Rows with a sum of counts less than this threshold will be removed.\n",
    "                             Default is 2, so sums of 0 or 1 will be removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_csv_path, 'r', newline='') as infile, \\\n",
    "             open(output_csv_path, 'w', newline='') as outfile, \\\n",
    "             open(log_csv_path, 'w', newline='') as logfile:\n",
    "\n",
    "            csv_reader = csv.reader(infile)\n",
    "            csv_writer = csv.writer(outfile)\n",
    "            log_writer = csv.writer(logfile)\n",
    "\n",
    "            # Read the header\n",
    "            header = next(csv_reader, None)\n",
    "            if not header:\n",
    "                print(f\"Warning: Input file '{input_csv_path}' is empty or has no header.\")\n",
    "                return\n",
    "\n",
    "            # Write headers to output files\n",
    "            csv_writer.writerow(header)\n",
    "            log_writer.writerow(header)\n",
    "\n",
    "            # Identify columns to sum based on pattern:\n",
    "            # A capital letter followed by exactly two digits (e.g., A01, B12, Z99)\n",
    "            column_pattern = re.compile(r'^[A-Z]\\d{2}$')\n",
    "            sum_column_indices = []\n",
    "            for i, col_name in enumerate(header):\n",
    "                if column_pattern.match(col_name.strip()): # .strip() to handle potential whitespace\n",
    "                    sum_column_indices.append(i)\n",
    "\n",
    "            if not sum_column_indices:\n",
    "                print(f\"Warning: No columns matching the pattern '[A-Z]\\\\d{{2}}' (e.g., A01, B12) found in the header of '{input_csv_path}'.\")\n",
    "                print(\"Please ensure your column headers for summing are like 'A01', 'S34', etc.\")\n",
    "            else:\n",
    "                print(f\"Identified columns for summing: {[header[i] for i in sum_column_indices]}\")\n",
    "\n",
    "            rows_processed = 0\n",
    "            rows_kept = 0\n",
    "            rows_removed = 0\n",
    "\n",
    "            # Process each data row\n",
    "            for i, row in enumerate(csv_reader):\n",
    "                rows_processed += 1\n",
    "                current_sum = 0\n",
    "                try:\n",
    "                    for col_idx in sum_column_indices:\n",
    "                        if col_idx < len(row):\n",
    "                            value_str = row[col_idx].strip()\n",
    "                            if value_str:\n",
    "                                try:\n",
    "                                    current_sum += int(float(value_str))\n",
    "                                except ValueError:\n",
    "                                    print(f\"Warning: Could not convert '{row[col_idx]}' to an integer in row {i+2} (data row {i+1}), column '{header[col_idx]}'. Treating as 0 for sum.\")\n",
    "                        else:\n",
    "                            print(f\"Warning: Row {i+2} (data row {i+1}) is shorter than header. Missing data for expected sum column '{header[col_idx]}'.\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing data in row {i+2} (data row {i+1}): {row}. Error: {e}\")\n",
    "\n",
    "                if current_sum < sum_threshold:\n",
    "                    log_writer.writerow(row)\n",
    "                    rows_removed += 1\n",
    "                else:\n",
    "                    csv_writer.writerow(row)\n",
    "                    rows_kept += 1\n",
    "            \n",
    "            print(f\"\\nProcessing complete for '{input_csv_path}':\")\n",
    "            print(f\"  Total data rows processed (excluding header): {rows_processed}\")\n",
    "            print(f\"  Rows kept and written to '{output_csv_path}': {rows_kept}\")\n",
    "            print(f\"  Rows removed and written to '{log_csv_path}': {rows_removed}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_csv_path}' not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# --- Main execution point if running as a standalone Python script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Name of the input CSV file uploaded by the user\n",
    "    input_filename_script = \"otu_table_uclust_with_updated_taxonomy_05172025_78bats.csv\"\n",
    "    \n",
    "    # Determine the base name for output files\n",
    "    base_name_script = os.path.splitext(input_filename_script)[0]\n",
    "    \n",
    "    # Define output file paths\n",
    "    output_filename_script = f\"{base_name_script}_singletsRemoved_78bats.csv\"\n",
    "    log_filename_script = f\"{base_name_script}_removed_rows.log_78bats.csv\"\n",
    "\n",
    "    print(f\"Starting OTU table processing for: {input_filename_script} (as standalone script)\")\n",
    "    print(f\"Filtered output will be saved to: {output_filename_script}\")\n",
    "    print(f\"Removed rows will be logged to: {log_filename_script}\")\n",
    "    \n",
    "    process_otu_table(input_filename_script, output_filename_script, log_filename_script, sum_threshold=2)\n",
    "    \n",
    "    print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253373e-3223-45b3-abc7-1298acaf6022",
   "metadata": {},
   "source": [
    "# Filtering out Metazoa and No Classification Taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "086a85eb-34a0-4c7c-ac3b-42ea9c17e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CSV filtering for: otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats.csv\n",
      "Rows where 'Domain' contains ['Metazoa', 'No Classification'] (case-insensitive) will be removed.\n",
      "Filtered data will be saved to: otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains.csv\n",
      "Removed rows will be logged to: otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_removed_domain_rows.log.csv\n",
      "\n",
      "Processing complete for 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats.csv':\n",
      "  Total data rows processed (excluding header): 1939\n",
      "  Rows kept and written to 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains.csv': 1511\n",
      "  Rows removed and written to 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_removed_domain_rows.log.csv': 428\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def filter_csv_by_domain(input_csv_path, filtered_csv_path, log_csv_path, domain_column_name=\"Domain\", values_to_remove=None):\n",
    "    \"\"\"\n",
    "    Filters a CSV file by removing rows where the specified domain column contains certain values.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file.\n",
    "        filtered_csv_path (str): Path to write the filtered CSV data (rows kept).\n",
    "        log_csv_path (str): Path to write the log of removed rows.\n",
    "        domain_column_name (str): The name of the column to check for domain values.\n",
    "        values_to_remove (list): A list of string values. If the domain column matches\n",
    "                                 any of these values (case-insensitive), the row is removed.\n",
    "    \"\"\"\n",
    "    if values_to_remove is None:\n",
    "        values_to_remove = [\"metazoa\", \"no classification\"] # Default values, made lowercase for case-insensitive comparison\n",
    "    else:\n",
    "        # Ensure comparison values are lowercase for case-insensitivity\n",
    "        values_to_remove = [str(val).lower() for val in values_to_remove]\n",
    "\n",
    "    rows_processed = 0\n",
    "    rows_kept = 0\n",
    "    rows_removed = 0\n",
    "    domain_col_idx = -1\n",
    "\n",
    "    try:\n",
    "        with open(input_csv_path, 'r', newline='', encoding='utf-8') as infile, \\\n",
    "             open(filtered_csv_path, 'w', newline='', encoding='utf-8') as outfile_filtered, \\\n",
    "             open(log_csv_path, 'w', newline='', encoding='utf-8') as outfile_log:\n",
    "\n",
    "            csv_reader = csv.reader(infile)\n",
    "            csv_writer_filtered = csv.writer(outfile_filtered)\n",
    "            csv_writer_log = csv.writer(outfile_log)\n",
    "\n",
    "            # Read and write the header\n",
    "            header = next(csv_reader, None)\n",
    "            if not header:\n",
    "                print(f\"Error: Input file '{input_csv_path}' is empty or has no header.\")\n",
    "                return\n",
    "\n",
    "            csv_writer_filtered.writerow(header)\n",
    "            csv_writer_log.writerow(header)\n",
    "\n",
    "            # Find the index of the domain column\n",
    "            try:\n",
    "                domain_col_idx = header.index(domain_column_name)\n",
    "            except ValueError:\n",
    "                print(f\"Error: Column '{domain_column_name}' not found in the header of '{input_csv_path}'.\")\n",
    "                print(f\"Available columns: {header}\")\n",
    "                # Write all rows to filtered output if domain column is not found, as filtering cannot be applied\n",
    "                for row in csv_reader:\n",
    "                    csv_writer_filtered.writerow(row)\n",
    "                    rows_processed +=1\n",
    "                    rows_kept +=1\n",
    "                print(f\"\\nSince '{domain_column_name}' column was not found, all {rows_processed} data rows were written to '{filtered_csv_path}'.\")\n",
    "                return\n",
    "\n",
    "            # Process each data row\n",
    "            for row in csv_reader:\n",
    "                rows_processed += 1\n",
    "                try:\n",
    "                    domain_value = row[domain_col_idx].strip().lower() # Get value and make it lowercase\n",
    "                    \n",
    "                    # Check if the domain value is one of the values to remove\n",
    "                    # This also handles cases where the domain might be part of a larger string,\n",
    "                    # e.g. \"Metazoa (kingdom)\" would match \"metazoa\" if \"metazoa\" is in values_to_remove\n",
    "                    # For exact match, use: if domain_value in values_to_remove:\n",
    "                    should_remove = False\n",
    "                    for removal_term in values_to_remove:\n",
    "                        if removal_term in domain_value: # Check if the term is a substring\n",
    "                            should_remove = True\n",
    "                            break\n",
    "                    \n",
    "                    if should_remove:\n",
    "                        csv_writer_log.writerow(row)\n",
    "                        rows_removed += 1\n",
    "                    else:\n",
    "                        csv_writer_filtered.writerow(row)\n",
    "                        rows_kept += 1\n",
    "                except IndexError:\n",
    "                    # This might happen if a row has fewer columns than the header\n",
    "                    print(f\"Warning: Row {rows_processed + 1} has an unexpected number of columns. Skipping.\")\n",
    "                    # Optionally, write this problematic row to the log or a separate error file\n",
    "                    csv_writer_log.writerow(row + [\"Error: Malformed row\"]) \n",
    "                    rows_removed +=1\n",
    "\n",
    "\n",
    "            print(f\"\\nProcessing complete for '{input_csv_path}':\")\n",
    "            print(f\"  Total data rows processed (excluding header): {rows_processed}\")\n",
    "            print(f\"  Rows kept and written to '{filtered_csv_path}': {rows_kept}\")\n",
    "            print(f\"  Rows removed and written to '{log_csv_path}': {rows_removed}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_csv_path}' not found. Please check the file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Main execution point of the script ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Name of the input CSV file uploaded by the user\n",
    "    input_filename = \"otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats.csv\"\n",
    "    \n",
    "    # Determine the base name for output files\n",
    "    base_name_input = os.path.splitext(input_filename)[0]\n",
    "    \n",
    "    # Define output file paths\n",
    "    filtered_output_filename = f\"{base_name_input}_filtered_domains.csv\"\n",
    "    log_filename = f\"{base_name_input}_removed_domain_rows.log.csv\"\n",
    "\n",
    "    # Define the specific values in the \"Domain\" column that should trigger row removal\n",
    "    # The comparison will be case-insensitive and will check if these terms are *contained*\n",
    "    # within the domain string. For exact matches, the logic inside the function would need adjustment.\n",
    "    domains_to_remove = [\"Metazoa\", \"No Classification\"] # The function will handle case-insensitivity\n",
    "\n",
    "    print(f\"Starting CSV filtering for: {input_filename}\")\n",
    "    print(f\"Rows where 'Domain' contains {domains_to_remove} (case-insensitive) will be removed.\")\n",
    "    print(f\"Filtered data will be saved to: {filtered_output_filename}\")\n",
    "    print(f\"Removed rows will be logged to: {log_filename}\")\n",
    "    \n",
    "    filter_csv_by_domain(input_filename, \n",
    "                         filtered_output_filename, \n",
    "                         log_filename, \n",
    "                         domain_column_name=\"Domain\", \n",
    "                         values_to_remove=domains_to_remove)\n",
    "    \n",
    "    print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd083d3-74cd-442d-8742-93f3de19cfee",
   "metadata": {},
   "source": [
    "# Combining like Genera together \n",
    "- combines all like genera together in one row\n",
    "- leaves the blank columns\n",
    "- new OTU_ID reads like sum_denovo####, denovo####, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "942dcec3-4de7-4d63-ade4-9e7f63fe692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CSV processing for: otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains.csv\n",
      "Rows with blank Genus will be kept as is at the end of the file.\n",
      "Output will be saved to: otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains_combined_by_genus_keep_blanks.csv\n",
      "Combination log (for non-blank Genus) will be saved to: otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains_genus_combination_log_keep_blanks.txt\n",
      "Successfully loaded 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains.csv'. Shape: (1511, 89)\n",
      "Assuming 'OTU_ID' is the OTU ID column (first column).\n",
      "Identified numeric sample columns for summation: ['A01', 'A02', 'A03', 'A04', 'A05', 'A08', 'A09', 'A10', 'A12', 'B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B08', 'B09', 'B10', 'B12', 'C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C12', 'D01', 'D02', 'D03', 'D04', 'D05', 'D06', 'D07', 'D09', 'D10', 'D11', 'D12', 'E01', 'E02', 'E03', 'E04', 'E06', 'E07', 'E08', 'E09', 'E10', 'E11', 'E12', 'F01', 'F02', 'F03', 'F04', 'F06', 'F07', 'F08', 'F09', 'F10', 'F11', 'G01', 'G02', 'G03', 'G04', 'G07', 'G08', 'G09', 'G10', 'G11', 'H01', 'H02', 'H03', 'H04', 'H07', 'H08', 'H09', 'H11']\n",
      "Identified other taxonomy columns to carry over: ['Domain', 'Phylum', 'Class', 'Order', 'Family', 'Species']\n",
      "421 rows with blank Genus will be kept as is and appended.\n",
      "1090 rows with non-blank Genus will be processed for combination.\n",
      "\n",
      "Starting aggregation for non-blank Genus entries by 'Genus'...\n",
      "Aggregation for non-blank Genus entries complete.\n",
      "\n",
      "Combined data (including rows with blank Genus) saved to 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains_combined_by_genus_keep_blanks.csv'. Shape: (617, 89)\n",
      "Log of combined OTU IDs (for non-blank Genus) saved to 'otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains_genus_combination_log_keep_blanks.txt'\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def combine_rows_by_genus(input_csv_path, output_csv_path, log_file_path,\n",
    "                          genus_col_name=\"Genus\", otu_id_col_identifier=None):\n",
    "    \"\"\"\n",
    "    Combines rows in a CSV file based on identical entries in the 'Genus' column.\n",
    "    Numeric sample columns are summed. OTU IDs are aggregated.\n",
    "    Rows with blank Genus entries are kept as is and appended to the output.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file.\n",
    "        output_csv_path (str): Path to save the combined CSV data.\n",
    "        log_file_path (str): Path to save the log of combined OTU IDs.\n",
    "        genus_col_name (str): Name of the column containing Genus information.\n",
    "        otu_id_col_identifier (str, optional): Name of the OTU ID column. \n",
    "                                             If None, assumes the first column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        print(f\"Successfully loaded '{input_csv_path}'. Shape: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_csv_path}' not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Input CSV is empty. Nothing to process.\")\n",
    "        return\n",
    "\n",
    "    # Determine OTU ID column\n",
    "    if otu_id_col_identifier and otu_id_col_identifier in df.columns:\n",
    "        actual_otu_id_col = otu_id_col_identifier\n",
    "    elif df.columns[0]: \n",
    "        actual_otu_id_col = df.columns[0]\n",
    "        print(f\"Assuming '{actual_otu_id_col}' is the OTU ID column (first column).\")\n",
    "    else:\n",
    "        print(\"Error: Could not determine OTU ID column.\")\n",
    "        return\n",
    "\n",
    "    if genus_col_name not in df.columns:\n",
    "        print(f\"Error: Genus column '{genus_col_name}' not found in the CSV.\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return\n",
    "\n",
    "    # Identify numeric sample columns (e.g., A01, B12)\n",
    "    sample_col_pattern = re.compile(r'^[A-Z]\\d{2}$')\n",
    "    numeric_sample_cols = [col for col in df.columns if sample_col_pattern.match(col)]\n",
    "    if not numeric_sample_cols:\n",
    "        print(\"Warning: No numeric sample columns matching pattern '[A-Z]\\\\d{2}' found. Summation might not work as expected.\")\n",
    "    else:\n",
    "        print(f\"Identified numeric sample columns for summation: {numeric_sample_cols}\")\n",
    "        for col in numeric_sample_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "    standard_taxonomy_cols = [\"Domain\", \"Phylum\", \"Class\", \"Order\", \"Family\", \"Species\"]\n",
    "    other_taxonomy_cols = [col for col in standard_taxonomy_cols if col in df.columns and col != genus_col_name]\n",
    "    print(f\"Identified other taxonomy columns to carry over: {other_taxonomy_cols}\")\n",
    "\n",
    "    # Prepare Genus column and identify blank entries\n",
    "    df[genus_col_name] = df[genus_col_name].astype(str).str.strip() # Ensure string type and strip whitespace\n",
    "    blank_genus_indicators = ['', 'nan', 'none', 'na', '<na>'] # Made lowercase for consistent check\n",
    "    \n",
    "    # Separate rows with blank Genus from those to be combined\n",
    "    is_blank_genus_mask = df[genus_col_name].str.lower().isin(blank_genus_indicators) | df[genus_col_name].isnull()\n",
    "    df_blank_genus = df[is_blank_genus_mask].copy()\n",
    "    df_to_combine = df[~is_blank_genus_mask].copy()\n",
    "\n",
    "    print(f\"{len(df_blank_genus)} rows with blank Genus will be kept as is and appended.\")\n",
    "    print(f\"{len(df_to_combine)} rows with non-blank Genus will be processed for combination.\")\n",
    "\n",
    "    df_combined_aggregated = pd.DataFrame() # Initialize empty DataFrame for aggregated results\n",
    "\n",
    "    if not df_to_combine.empty:\n",
    "        agg_funcs = {}\n",
    "        def aggregate_otu_ids(x):\n",
    "            ids = \", \".join(sorted(list(set(x.astype(str)))))\n",
    "            return f\"sum of {ids}\" if ids else \"N/A\"\n",
    "        \n",
    "        agg_funcs[actual_otu_id_col] = aggregate_otu_ids\n",
    "\n",
    "        for col in numeric_sample_cols:\n",
    "            agg_funcs[col] = 'sum'\n",
    "        for col in other_taxonomy_cols:\n",
    "            agg_funcs[col] = 'first'\n",
    "\n",
    "        print(f\"\\nStarting aggregation for non-blank Genus entries by '{genus_col_name}'...\")\n",
    "        df_to_combine[actual_otu_id_col] = df_to_combine[actual_otu_id_col].astype(str)\n",
    "\n",
    "        try:\n",
    "            df_combined_aggregated = df_to_combine.groupby(genus_col_name, as_index=False).agg(agg_funcs)\n",
    "            print(\"Aggregation for non-blank Genus entries complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during aggregation: {e}\")\n",
    "            print(\"Aggregation functions defined:\", agg_funcs)\n",
    "            print(\"Columns in df_to_combine:\", df_to_combine.columns.tolist())\n",
    "            # If aggregation fails, we might still want to output the blank genus rows\n",
    "            # For now, we'll let it proceed to concat, df_combined_aggregated will be empty.\n",
    "    else:\n",
    "        print(\"No non-blank Genus entries to aggregate.\")\n",
    "\n",
    "    # Concatenate aggregated data with rows that had blank Genus\n",
    "    # Ensure columns are aligned; df_blank_genus has original columns.\n",
    "    # df_combined_aggregated might have fewer columns if some taxonomy levels were all NaN for a genus.\n",
    "    # We need to make sure the final output has all original columns in the original order.\n",
    "    \n",
    "    final_df_list = []\n",
    "    if not df_combined_aggregated.empty:\n",
    "        final_df_list.append(df_combined_aggregated)\n",
    "    if not df_blank_genus.empty:\n",
    "        # Ensure df_blank_genus has the same columns as the original df for proper concatenation order\n",
    "        # If df_combined_aggregated is empty, the output columns should match original df\n",
    "        # If df_combined_aggregated is not empty, its columns define the primary structure for aggregated part\n",
    "        final_df_list.append(df_blank_genus)\n",
    "\n",
    "    if not final_df_list:\n",
    "        print(\"No data to write to output file (neither aggregated nor blank genus rows).\")\n",
    "        # Create empty file with headers if original df was not empty\n",
    "        if not df.empty:\n",
    "            pd.DataFrame(columns=df.columns).to_csv(output_csv_path, index=False)\n",
    "            with open(log_file_path, 'w', encoding='utf-8') as log_f:\n",
    "                log_f.write(\"Genus,Combined_OTU_IDs_Raw\\n\") # Log will be empty if no combinations\n",
    "        return\n",
    "\n",
    "    df_final_output = pd.concat(final_df_list, ignore_index=True)\n",
    "\n",
    "    # Reorder columns to match the original input as much as possible\n",
    "    # Use original df's column order as the reference\n",
    "    original_columns = df.columns.tolist()\n",
    "    # Filter to columns present in the final output, maintaining original order\n",
    "    final_ordered_columns = [col for col in original_columns if col in df_final_output.columns]\n",
    "    # Add any new columns created during aggregation (shouldn't be any if logic is right)\n",
    "    # or columns that might have been only in one part of the concat (e.g. only in blank_genus part)\n",
    "    for col in df_final_output.columns:\n",
    "        if col not in final_ordered_columns:\n",
    "            final_ordered_columns.append(col)\n",
    "            \n",
    "    df_final_output = df_final_output[final_ordered_columns]\n",
    "\n",
    "\n",
    "    try:\n",
    "        df_final_output.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nCombined data (including rows with blank Genus) saved to '{output_csv_path}'. Shape: {df_final_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving final combined CSV: {e}\")\n",
    "\n",
    "    # Write the log file for combined OTUs (only for non-blank Genus entries that were combined)\n",
    "    try:\n",
    "        with open(log_file_path, 'w', encoding='utf-8') as log_f:\n",
    "            log_f.write(\"Genus,Combined_OTU_IDs_Raw\\n\")\n",
    "            if not df_to_combine.empty: # Log only if there was data to combine\n",
    "                for genus, group_df in df_to_combine.groupby(genus_col_name):\n",
    "                    otu_ids_for_log = \", \".join(sorted(list(set(group_df[actual_otu_id_col].astype(str)))))\n",
    "                    log_f.write(f\"\\\"{genus}\\\",\\\"{otu_ids_for_log}\\\"\\n\")\n",
    "        print(f\"Log of combined OTU IDs (for non-blank Genus) saved to '{log_file_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing log file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"otu_table_uclust_with_updated_taxonomy_05172025_78bats_singletsRemoved_78bats_filtered_domains.csv\"\n",
    "    \n",
    "    base_name_input = os.path.splitext(input_filename)[0]\n",
    "    \n",
    "    output_filename = f\"{base_name_input}_combined_by_genus_keep_blanks.csv\" # Updated output name\n",
    "    log_filename = f\"{base_name_input}_genus_combination_log_keep_blanks.txt\" # Updated log name\n",
    "\n",
    "    print(f\"Starting CSV processing for: {input_filename}\")\n",
    "    print(f\"Rows with blank Genus will be kept as is at the end of the file.\")\n",
    "    print(f\"Output will be saved to: {output_filename}\")\n",
    "    print(f\"Combination log (for non-blank Genus) will be saved to: {log_filename}\")\n",
    "\n",
    "    combine_rows_by_genus(input_filename, \n",
    "                          output_filename, \n",
    "                          log_filename,\n",
    "                          genus_col_name=\"Genus\",\n",
    "                          otu_id_col_identifier=None) \n",
    "    \n",
    "    print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b706b-d547-4f02-bfcc-61a9120ed910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
